{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b69724b",
   "metadata": {},
   "source": [
    "### Supervised Fine-Tuning\n",
    "\n",
    "This notebook demonstrates three widely used methods for fine-tuning models from the Qwen-2.5 series. It includes steps for data preparation and tokenization tailored specifically to Qwen-2.5 models, though the underlying concepts are applicable to other model series as well.\n",
    "\n",
    "The showcased task is a Hinglish style transfer problem, aiming to strike a balance between model size and generation quality (as evaluated in `evaluation.ipynb`). This notebook focuses solely on the fine-tuning process and saving the resulting modelâ€”either as a fully fine-tuned model or as lightweight adapters using PEFT techniques.\n",
    "\n",
    "#### Fine-Tuning Methods Covered:\n",
    "\n",
    "* Full fine-tuning\n",
    "* LoRA (Low-Rank Adaptation)\n",
    "* QLoRA (Quantized LoRA)\n",
    "\n",
    "\n",
    "<img src=\"./assets/SFT.png\" alt=\"SFT_techniques\" height=\"500px\"/>\n",
    "\n",
    "*Date : 20th April 2025 | Author : Prakhar Bhartiya*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7ee66e",
   "metadata": {},
   "source": [
    "### Environment Setup and Package Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ded79",
   "metadata": {},
   "source": [
    "Before starting, install the necessary packages based on your system configuration.\n",
    "\n",
    "PyTorch Installation\n",
    "\n",
    "- For Windows/Linux with NVIDIA GPU (e.g., RTX 30/40/50 series or A100):Install the CUDA-enabled version of PyTorch.\n",
    "    ```bash\n",
    "    # cuXXX at the end denotes cuda version. At time of writing 12.6 is the latest official stable cuda-pytorch\n",
    "    !pip install -qU torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126 \n",
    "    ```\n",
    "\n",
    "- For Apple Silicon (e.g., M1/M2):Use the PyTorch build with Metal backend acceleration.\n",
    "    ```bash\n",
    "    !pip install torch\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Nvidia - GPUs]\n",
    "# !pip install -qU torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126 \n",
    "\n",
    "# [Apple Silicon]\n",
    "# !pip install -qU torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc14735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest of the packages\n",
    "# !pip install -qU transformers datasets accelerate peft matplotlib seaborn trl hf_xet bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18547475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Model ID from Hugging Face - https://huggingface.co/Qwen/Qwen2.5-3B-Instruct\n",
    "MODEL_ID = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "# https://huggingface.co/datasets/prakharb01/Synthetic-Hinglish-Finetuning-Dataset\n",
    "DATASET_ID = \"prakharb01/Synthetic-Hinglish-Finetuning-Dataset\"\n",
    "\n",
    "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
    "print(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea37572",
   "metadata": {},
   "source": [
    "### Loading dataset - From Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5b974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [HuggingFace]\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset directly from the Hugging Face Hub\n",
    "dataset = load_dataset(DATASET_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb90338",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a651a",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    raw_dataset = dataset['train']\n",
    "    print(f\"Loaded dataset with {len(raw_dataset)} conversations.\")\n",
    "    print(\"Sample Conversation Structure:\", raw_dataset[0]['conversation'])\n",
    "    DATASET_CONVERSATION_COLUMN = \"conversation\" # Define the column name\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing dataset : {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46fb636",
   "metadata": {},
   "source": [
    "### Data Formatting & Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1a4e8d",
   "metadata": {},
   "source": [
    "![tokenization_meaning](https://miro.medium.com/v2/resize:fit:1400/1*HBCn3f7lf3ITtga3Dc20Tw.png)\n",
    "\n",
    "*Image credit : [ypmanohar](https://medium.com/@ypmanohar.iitm/demystifying-tokenization-the-building-blocks-of-language-ai-5c1dbd5f4343)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6ff56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under the hood : How does it look?\n",
    "\n",
    "# Qwen2.5 Models Tokenizer Pattern\n",
    "# https://huggingface.co/Qwen/Qwen2.5-3B-Instruct/blob/main/tokenizer_config.json\n",
    "\n",
    "print(\"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- messages[0]['content'] }}\\n    {%- else %}\\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\\n    {%- endif %}\\n    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\\n    {%- else %}\\n        {{- '<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) or (message.role == \\\"assistant\\\" and not message.tool_calls) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {{- '<|im_start|>' + message.role }}\\n        {%- if message.content %}\\n            {{- '\\\\n' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n            {{- tool_call.name }}\\n            {{- '\\\", \\\"arguments\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- '}\\\\n</tool_call>' }}\\n        {%- endfor %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f386e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-Llama3.2 model series\n",
    "\n",
    "print(\"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\\\"%d %b %Y\\\") %}\\n    {%- else %}\\n        {%- set date_string = \\\"26 Jul 2024\\\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \\\"\\\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \\\"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\\\" }}\\n{%- if tools is not none %}\\n    {{- \\\"Environment: ipython\\\\n\\\" }}\\n{%- endif %}\\n{{- \\\"Cutting Knowledge Date: December 2023\\\\n\\\" }}\\n{{- \\\"Today Date: \\\" + date_string + \\\"\\\\n\\\\n\\\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \\\"You have access to the following functions. To call a function, please respond with JSON for a function call.\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \\\"<|eot_id|>\\\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n{%- endif %}\\n    {{- '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\\\n\\\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot_id|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim + '<|eot_id|>' }}\\n    {%- elif 'tool_calls' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\\\"This model only supports single tool-calls at once!\\\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n        {{- '{\\\"name\\\": \\\"' + tool_call.name + '\\\", ' }}\\n        {{- '\\\"parameters\\\": ' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \\\"}\\\" }}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c698bb7c",
   "metadata": {},
   "source": [
    "<img src=\"./assets/template.png\" alt=\"chat-template\" height=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee6591d-ecb1-4f13-ad68-43bcda5b67a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** IMPORTANT: Adjust MAX_SEQ_LENGTH based on VRAM and conversation length *****\n",
    "\n",
    "# Typical value : 512, 1024 or 2048 if possible. Longer context needs more VRAM.\n",
    "MAX_SEQ_LENGTH = 512 # Context window\n",
    "\n",
    "# System prompt\n",
    "SYSTEM_PROMPT = \"You are a helpful college friend who talks only in Hinglish (a mix of Hindi and English used in urban India). Be casual, friendly, and use common Hinglish slang. Do not use formal Hindi or pure English.\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Set pad_token to eos_token: {tokenizer.pad_token}\")\n",
    "\n",
    "\n",
    "# Define markers based on Qwen template structure\n",
    "# These are used to identify assistant response boundaries AFTER template application\n",
    "# Note: Encoding these might yield slightly different IDs depending on context,\n",
    "# so finding them robustly in the full sequence is key.\n",
    "assistant_start_marker_str = \"<|im_start|>assistant\\n\"\n",
    "assistant_end_marker_str = \"<|im_end|>\"\n",
    "\n",
    "# Tokenize the markers to get their IDs\n",
    "# We encode them standalone first, but be aware context can affect tokenization slightly.\n",
    "# It's safer to search for the *sequence* of tokens.\n",
    "assistant_start_marker_ids = tokenizer.encode(assistant_start_marker_str, add_special_tokens=False)\n",
    "assistant_end_marker_ids = tokenizer.encode(assistant_end_marker_str, add_special_tokens=False)\n",
    "\n",
    "# Helper function to find sublist indices\n",
    "def find_sublist_indices(main_list, sub_list):\n",
    "    indices = []\n",
    "    len_sub = len(sub_list)\n",
    "    if len_sub == 0:\n",
    "        return indices\n",
    "    for i in range(len(main_list) - len_sub + 1):\n",
    "        if main_list[i:i+len_sub] == sub_list:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "def format_and_tokenize_conversation(example):\n",
    "    \"\"\"\n",
    "    Formats a conversation list using the chat template, tokenizes it,\n",
    "    and creates labels masking non-assistant parts.\n",
    "    \"\"\"\n",
    "    conversation = example[DATASET_CONVERSATION_COLUMN]\n",
    "\n",
    "    # 1. Add system prompt if not present\n",
    "    if not conversation or conversation[0]['role'] != 'system':\n",
    "        conversation.insert(0, {\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
    "\n",
    "    # 2. Apply chat template to the full conversation\n",
    "    # We don't add generation prompt here; we have the full conversation\n",
    "    # Truncation=False initially, we'll truncate manually later if needed\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=False,\n",
    "        padding=False, # No padding yet\n",
    "        truncation=False # No truncation yet\n",
    "    )\n",
    "\n",
    "    # 3. Create labels: Initialize with -100, then unmask assistant parts\n",
    "    labels = [-100] * len(input_ids)\n",
    "\n",
    "    # Find all start and end markers for assistant turns\n",
    "    start_indices = find_sublist_indices(input_ids, assistant_start_marker_ids)\n",
    "    end_indices = find_sublist_indices(input_ids, assistant_end_marker_ids)\n",
    "\n",
    "    # Iterate through assistant turns identified by markers\n",
    "    current_assistant_turn = 0\n",
    "    for start_marker_idx in start_indices:\n",
    "        # Find the corresponding end marker AFTER this start marker\n",
    "        # The assistant response tokens are between the end of the start marker\n",
    "        # and the beginning of the end marker.\n",
    "        assistant_content_start_index = start_marker_idx + len(assistant_start_marker_ids)\n",
    "\n",
    "        # Find the *first* end marker that appears after assistant_content_start_index\n",
    "        possible_end_marker_indices = [idx for idx in end_indices if idx >= assistant_content_start_index]\n",
    "        if possible_end_marker_indices:\n",
    "            assistant_content_end_index = possible_end_marker_indices[0]\n",
    "\n",
    "            # Unmask the labels for the assistant's content tokens\n",
    "            for i in range(assistant_content_start_index, assistant_content_end_index):\n",
    "                if i < len(labels): # Boundary check\n",
    "                    labels[i] = input_ids[i]\n",
    "            current_assistant_turn += 1\n",
    "        else:\n",
    "            # This shouldn't happen if the template is applied correctly, but good to know\n",
    "            print(f\"Warning: Could not find matching end marker for assistant turn starting around index {start_marker_idx}.\")\n",
    "\n",
    "\n",
    "    # 4. Truncate sequences if they exceed max length\n",
    "    if len(input_ids) > MAX_SEQ_LENGTH:\n",
    "        input_ids = input_ids[:MAX_SEQ_LENGTH]\n",
    "        labels = labels[:MAX_SEQ_LENGTH]\n",
    "\n",
    "    # 5. Pad sequences\n",
    "    pad_len = MAX_SEQ_LENGTH - len(input_ids)\n",
    "    input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n",
    "    labels = labels + [-100] * pad_len # Pad labels with -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": [1] * (MAX_SEQ_LENGTH - pad_len) + [0] * pad_len, # Create attention mask\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "# Apply the formatting and tokenization function\n",
    "# Ensure original 'conversation' column is removed if not needed by Trainer\n",
    "processed_dataset = raw_dataset.map(\n",
    "    format_and_tokenize_conversation,\n",
    "    remove_columns=[DATASET_CONVERSATION_COLUMN] # Remove original column\n",
    ")\n",
    "\n",
    "print(\"\\nSample processed example:\")\n",
    "sample = processed_dataset[0]\n",
    "print(\"Input IDs:\", sample['input_ids'][:50], \"...\")\n",
    "print(\"Labels:\", sample['labels'][:50], \"...\") # Should have -100 for non-assistant parts\n",
    "print(\"Attention Mask:\", sample['attention_mask'][:50], \"...\")\n",
    "\n",
    "# Decode to verify masking\n",
    "decoded_input = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)\n",
    "print(\"\\nDecoded Input (Full):\", decoded_input)\n",
    "decoded_labels_display = tokenizer.decode([l if l != -100 else tokenizer.pad_token_id for l in sample['labels']], skip_special_tokens=False)\n",
    "print(\"\\nDecoded Labels (Unmasked Parts Only):\", decoded_labels_display.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083fcd94",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function for plotting ---\n",
    "def plot_loss(history, title, filename):\n",
    "    if history is None:\n",
    "        print(\"No training history found.\")\n",
    "        return\n",
    "\n",
    "    train_loss = [log.get('loss') for log in history if 'loss' in log]\n",
    "    eval_loss = [log.get('eval_loss') for log in history if 'eval_loss' in log] # Add if eval is run\n",
    "    steps = [log.get('step') for log in history if 'loss' in log]\n",
    "    eval_steps = [log.get('step') for log in history if 'eval_loss' in log] # Add if eval is run\n",
    "\n",
    "    if not train_loss or not steps:\n",
    "        print(\"Training loss data not found in history.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x=steps, y=train_loss, label='Training Loss')\n",
    "    if eval_loss and eval_steps:\n",
    "        sns.lineplot(x=eval_steps, y=eval_loss, label='Validation Loss')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{filename}.png\")\n",
    "    plt.show()\n",
    "\n",
    "def select_device():\n",
    "    device_available = \"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device_available = \"cuda\"\n",
    "    elif torch.mps.is_available():\n",
    "        device_available = \"mps\"\n",
    "    else:\n",
    "        device_available = \"cpu\"\n",
    "    \n",
    "    return torch.device(device_available)\n",
    "\n",
    "# --- Helper function for generation ---\n",
    "def generate_conversational_response(model, tokenizer, conversation_history, device='cuda'):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "\n",
    "    # Ensure system prompt is present\n",
    "    if not conversation_history or conversation_history[0].get(\"role\") != \"system\":\n",
    "         conversation_history.insert(0, {\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        conversation_history,\n",
    "        add_generation_prompt=True, # Add prompt for assistant's turn\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response_ids = outputs[0][input_ids.shape[-1]:]\n",
    "    response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Add assistant response to history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    return response, conversation_history # Return response and updated history\n",
    "\n",
    "# Determine device\n",
    "device = select_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cpu':\n",
    "    print(f\"Warning: Fine-tuning {MODEL_ID} model on CPU will be extremely slow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e10469f",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a50c33",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./assets/matrix_decomposition.png\" alt=\"matrix_decomposition\" height=\"500px\"/>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"./assets/lora_in_action.png\" alt=\"lora\" width=\"400px\"/>\n",
    "<img src=\"./assets/lora_in_action_2.png\" alt=\"lora2\" width=\"400px\"/>\n",
    "<img src=\"./assets/lora_merge.png\" alt=\"lora_merge\" width=\"400px\"/>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"./assets/lora_axis.png\" alt=\"lora_axis\" height=\"500px\"/>\n",
    "\n",
    "*Images Credit : [Edward Hu](https://youtu.be/DhRoTONcyZE)*\n",
    "\n",
    "*Simple brief explation video about LoRA's instuition from its Author*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ab106",
   "metadata": {},
   "source": [
    "![Typical_hyper_params](./assests/typical_params.png)\n",
    "\n",
    "*Image Credit : [Maxime Labonne (Liquid AI)](https://youtu.be/_HfdncCbMOE)*\n",
    "\n",
    "*Highly recommend this video, for learning about post-training + finetuning concepts and industry insights*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d582b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Args ---\n",
    "# Adjust these based on your hardware (GPU VRAM)\n",
    "# NOTE : These are starting points, might need tuning\n",
    "\n",
    "NUM_EPOCHS = 5 # Start with 1 epoch, increase if needed : People use 3-5 depending upon dataset size\n",
    "LEARNING_RATE_FULL = 2e-5\n",
    "LEARNING_RATE_LORA = 5e-4 # LoRA often uses a higher LR\n",
    "\n",
    "OUTPUT_DIR_BASE = f\"{MODEL_NAME}_hinglish_finetune\"\n",
    "\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 4 # Reduce if OOM errors occur\n",
    "GRADIENT_ACCUMULATION_STEPS = 2 # Increase effective batch size\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 1 # Always set to 1\n",
    "\n",
    "\n",
    "OPTIMIZER_LoRA = \"adamw_torch\" # Goog default in general - AdamW\n",
    "OPTIMIZER_QLoRA = \"paged_adamw_8bit\" # Good default for QLoRA\n",
    "\n",
    "LR_SCHEDULER = \"cosine\" # https://huggingface.co/docs/transformers/main/en/main_classes/optimizer_schedules#schedules\n",
    "\n",
    "WARMUP_RATIO = 0.03 # Ratio of total training steps used for a linear warmup from 0 to learning_rate | Default : 0.0 i.e. no warmup\n",
    "\n",
    "LOGGING_STEPS = 10\n",
    "SAVE_STEPS = 200 # Save checkpoints periodically\n",
    "SAVE_TOTAL_LIMIT = 2 # Keep only the last few checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdcabd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PEFT Configs ---\n",
    "\n",
    "# --- LoRA Config ---\n",
    "LORA_R = 8 # Rank\n",
    "LORA_ALPHA = 16 # Alpha scaling\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "# Check model architecture to confirm target modules if possible\n",
    "# Common modules for Qwen-like models:\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\",\n",
    "]\n",
    "\n",
    "# --- QLoRA Config ---\n",
    "# Ensures loading in 4-bit for QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", # Recommended quantization type\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for computation if supported (Ampere+ GPUs)\n",
    "    bnb_4bit_use_double_quant=True, # Use nested quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b9e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = processed_dataset.train_test_split(test_size=0.1, shuffle=True, seed=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b3803",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = split_dataset['train'], split_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac64273",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Full Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3643c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configs ---\n",
    "OUTPUT_DIR_FULL = os.path.join(OUTPUT_DIR_BASE, MODEL_NAME, \"full_finetune\")\n",
    "\n",
    "# [In depth information : https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments]\n",
    "\n",
    "training_args_full = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_FULL,\n",
    "    # num_train_epochs=NUM_EPOCHS,\n",
    "    num_train_epochs=1,\n",
    "    # per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE_FULL,\n",
    "    weight_decay=0.01,\n",
    "    optim= \"adamw_torch\", # Standard AdamW for full FT\n",
    "    lr_scheduler_type=LR_SCHEDULER,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit= 1,\n",
    "\n",
    "    # We are trying to use bf16 is its supported otherwise use fp16 i.e. fp16 = False ; bf16 = True\n",
    "    # Since MPS does support bf16 but doesn't have a boolean check function like `torch.cuda.is_bf16_supported()`, we are manually hardcoding it as True\n",
    "    fp16=(torch.cuda.is_available() or torch.mps.is_available()) and not (torch.cuda.is_bf16_supported() or True), # Use fp16 if bf16 not supported, automatically takes care of CUDA/MPS\n",
    "    bf16=(torch.cuda.is_available() and torch.cuda.is_bf16_supported()) or (torch.mps.is_available() and True), # Use bf16 if supported, rather using fp16, automatically takes care of CUDA/MPS\n",
    "   \n",
    "    eval_strategy=\"steps\", # Change to \"steps\" if eval_dataset is used and eval_steps is set\n",
    "    eval_steps=SAVE_STEPS, # Evaluate alongside saving\n",
    "    report_to=\"none\", # Disable wandb/tensorboard for this example\n",
    "    remove_unused_columns=False, # Important when using custom label logic\n",
    "    gradient_checkpointing=True, # Saves VRAM at cost of slower training\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# --- Load Model ---\n",
    "print(\"Loading base model for Full Fine-Tuning...\")\n",
    "# Note: Full FT requires significantly more VRAM. May not run on single consumer GPUs.\n",
    "# Consider using DeepSpeed or FSDP integration for larger scale FT.\n",
    "try:\n",
    "    model_full = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) or (torch.mps.is_available() and True) else torch.float16, # Load in appropriate dtype\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\" # Use accelerate's device mapping if multiple GPUs\n",
    "    )\n",
    "    model_full.to(device) # Move model to device if not using device_map\n",
    "    print(f\"Model VRAM Footprint (Full): {model_full.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "    # --- Trainer ---\n",
    "    trainer_full = Trainer(\n",
    "        model=model_full,\n",
    "        args=training_args_full,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset, # Optional, for validation loss\n",
    "        tokenizer=tokenizer,\n",
    "        # data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # --- Train ---\n",
    "    print(\"\\nStarting Full Fine-Tuning...\")\n",
    "    train_result_full = trainer_full.train()\n",
    "    print(\"Full Fine-Tuning finished.\")\n",
    "\n",
    "    # --- Save ---\n",
    "    print(f\"Saving Full Fine-Tuned model to {output_dir_full}...\")\n",
    "    trainer_full.save_model(output_dir_full)\n",
    "    # Save tokenizer too\n",
    "    tokenizer.save_pretrained(output_dir_full)\n",
    "    print(\"Model saved.\")\n",
    "\n",
    "    # --- Plot Loss ---\n",
    "    plot_loss(trainer_full.state.log_history, \"Full Fine-Tuning Training Loss\", f\"{output_dir_full}/full_ft_loss_plot\")\n",
    "\n",
    "    # --- Sample Generation [TODO : Need to modify to fit new conversation style] ---\n",
    "    # print(\"\\nGenerating sample response with Full Fine-Tuned model...\")\n",
    "    # fine_tuned_model_full = AutoModelForCausalLM.from_pretrained(\n",
    "    #     output_dir_full,\n",
    "    #     torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    #     trust_remote_code=True\n",
    "    # ).to(device)\n",
    "    # fine_tuned_model_full.config.pad_token_id = tokenizer.eos_token_id # Ensure pad token is set\n",
    "\n",
    "    # test_prompt = \"Exam kaisa tha?\"\n",
    "    # response = generate_conversational_response(fine_tuned_model_full, tokenizer, test_prompt, device)\n",
    "    # print(f\"Prompt: {test_prompt}\")\n",
    "    # print(f\"Full FT Response: {response}\")\n",
    "\n",
    "    # Clean up memory\n",
    "    del model_full\n",
    "    # del fine_tuned_model_full\n",
    "    del trainer_full\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR during Full Fine-Tuning: {e}\")\n",
    "    print(\"Full Fine-Tuning requires substantial GPU resources (VRAM). Skipping or ensure you have adequate hardware.\")\n",
    "    if 'model_full' in locals(): del model_full\n",
    "    if 'trainer_full' in locals(): del trainer_full\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9689cd7b",
   "metadata": {},
   "source": [
    "### LoRA Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b97c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configs ---\n",
    "OUTPUT_DIR_FULL = os.path.join(OUTPUT_DIR_BASE, MODEL_NAME, \"lora_finetune\")\n",
    "\n",
    "peft_config_lora = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "training_args_lora = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_FULL,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE_LORA, # Often higher for LoRA\n",
    "    weight_decay=0.01,\n",
    "    optim= OPTIMIZER_LoRA,\n",
    "    lr_scheduler_type=LR_SCHEDULER,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
    "    \n",
    "    # We are trying to use bf16 is its supported otherwise use fp16 i.e. fp16 = False ; bf16 = True\n",
    "    # Since MPS does support bf16 but doesn't have a boolean check function like `torch.cuda.is_bf16_supported()`, we are manually hardcoding it as True\n",
    "    fp16=(torch.cuda.is_available() or torch.mps.is_available()) and not (torch.cuda.is_bf16_supported() or True), # Use fp16 if bf16 not supported, automatically takes care of CUDA/MPS\n",
    "    bf16=(torch.cuda.is_available() and torch.cuda.is_bf16_supported()) or (torch.mps.is_available() and True), # Use bf16 if supported, rather using fp16, automatically takes care of CUDA/MPS\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=SAVE_STEPS,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True, # Useful for LoRA too\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# --- Load Model ---\n",
    "print(\"Loading base model for LoRA Fine-Tuning...\")\n",
    "try:\n",
    "    model_lora_base = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) or (torch.mps.is_available() and True) else torch.float16, # Load in appropriate dtype\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\" # Can use device_map here too\n",
    "    )\n",
    "    model_lora_base.to(device) # Move to device if not using device_map\n",
    "    model_lora_base.config.use_cache = False # Required for gradient checkpointing with PEFT\n",
    "    model_lora_base.enable_input_require_grads() # May be needed for some model versions\n",
    "\n",
    "    # --- Apply PEFT ---\n",
    "    print(\"Applying LoRA PEFT adapter...\")\n",
    "    model_lora = get_peft_model(model_lora_base, peft_config_lora)\n",
    "    model_lora.print_trainable_parameters()\n",
    "    print(f\"Model VRAM Footprint (LoRA Base + Adapters): {model_lora.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "    # --- Trainer ---\n",
    "    trainer_lora = Trainer(\n",
    "        model=model_lora,\n",
    "        args=training_args_lora,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # --- Train ---\n",
    "    print(\"\\nStarting LoRA Fine-Tuning...\")\n",
    "    train_result_lora = trainer_lora.train()\n",
    "    print(\"LoRA Fine-Tuning finished.\")\n",
    "\n",
    "    # --- Save Adapters ---\n",
    "    print(f\"Saving LoRA adapters to {output_dir_lora}...\")\n",
    "    trainer_lora.save_model(output_dir_lora) # Saves only adapters\n",
    "     # Save tokenizer too\n",
    "    tokenizer.save_pretrained(output_dir_lora)\n",
    "    print(\"Adapters saved.\")\n",
    "\n",
    "    # --- Plot Loss ---\n",
    "    plot_loss(trainer_lora.state.log_history, \"LoRA Fine-Tuning Training Loss\", f\"{output_dir_lora}/lora_ft_loss_plot\")\n",
    "\n",
    "    # --- Sample Generation ---\n",
    "    # Its a good idea, to see the generation in regular intervals, to see the alignment\n",
    "    print(\"\\nGenerating sample response with LoRA Fine-Tuned model...\")\n",
    "    # Load base model again and apply adapters\n",
    "    base_model_for_lora_inference = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) or (torch.mps.is_available() and True) else torch.float16, # Load in appropriate dtype\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\" # Or move manually to device\n",
    "    ).to(device)\n",
    "    base_model_for_lora_inference.config.pad_token_id = tokenizer.eos_token_id # Ensure pad token\n",
    "\n",
    "    lora_model_inference = PeftModel.from_pretrained(base_model_for_lora_inference, output_dir_lora)\n",
    "    # lora_model_inference = lora_model_inference.merge_and_unload() # Merge for faster inference (optional)\n",
    "\n",
    "    # --- Test Conversation ---\n",
    "    print(\"\\n--- Starting Test Conversation ---\")\n",
    "    current_conversation = []\n",
    "    user_prompt_1 = \"Hey, what's up? Free ho kya?\"\n",
    "    print(f\"User: {user_prompt_1}\")\n",
    "    current_conversation.append({\"role\": \"user\", \"content\": user_prompt_1})\n",
    "    response_1, current_conversation = generate_conversational_response(\n",
    "        lora_model_inference, tokenizer, current_conversation, device=device.type\n",
    "    )\n",
    "    print(f\"AI: {response_1}\")\n",
    "\n",
    "    user_prompt_2 = \"Bas class se nikla. Exam kaisa gaya tera?\"\n",
    "    print(f\"User: {user_prompt_2}\")\n",
    "    current_conversation.append({\"role\": \"user\", \"content\": user_prompt_2})\n",
    "    response_2, current_conversation = generate_conversational_response(\n",
    "        lora_model_inference, tokenizer, current_conversation, device=device.type\n",
    "    )\n",
    "    print(f\"AI: {response_2}\")\n",
    "    print(\"--- End Test Conversation ---\")\n",
    "\n",
    "    # Clean up memory\n",
    "    del model_lora_base\n",
    "    del model_lora\n",
    "    del base_model_for_lora_inference\n",
    "    del lora_model_inference\n",
    "    del trainer_lora\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR during LoRA Fine-Tuning: {e}\")\n",
    "    if 'model_lora_base' in locals(): del model_lora_base\n",
    "    if 'model_lora' in locals(): del model_lora\n",
    "    if 'base_model_for_lora_inference' in locals(): del base_model_for_lora_inference\n",
    "    if 'lora_model_inference' in locals(): del lora_model_inference\n",
    "    if 'trainer_lora' in locals(): del trainer_lora\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0f493",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### QLoRA Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf8de81",
   "metadata": {},
   "source": [
    "<p style=\"color:red\">QLoRA is quantized version of LoRA. Primary library `bitsandbytes` relies on CUDA for performing efficient 8-bit and 4-bit matrix operations.\n",
    "It uses NVIDIA-specific kernels (like cuBLASLt) for low-level optimized computations.\n",
    "There is no support for AMD ROCm, Apple Metal, or CPU-only environments as of now.</p>\n",
    "\n",
    "Warning you will see on top for non-NVIDIA system\n",
    "```\n",
    "UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
    "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
    "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configs ---\n",
    "if device.type != \"cuda\":\n",
    "    raise RuntimeError(f\"Cannot proceed: NVIDIA GPU required, but found '{device.type}'\")\n",
    "\n",
    "OUTPUT_DIR_FULL = os.path.join(OUTPUT_DIR_BASE, MODEL_NAME, \"qlora_finetune\")\n",
    "peft_config_qlora = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "training_args_qlora = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_FULL,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE_LORA, # Same LR as LoRA often works\n",
    "    weight_decay=0.01,\n",
    "    optim=OPTIMIZER_QLoRA, # Use paged optimizer for QLoRA efficiency\n",
    "    lr_scheduler_type=LR_SCHEDULER,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
    "    fp16=False, # QLoRA uses custom dtype logic via bitsandbytes\n",
    "    bf16=False, # QLoRA uses custom dtype logic via bitsandbytes\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=SAVE_STEPS,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True, # Highly recommended for QLoRA\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# --- Load Quantized Model ---\n",
    "print(\"Loading base model with 4-bit quantization for QLoRA...\")\n",
    "try:\n",
    "    model_qlora_base = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16 if bnb_config.bnb_4bit_compute_dtype == torch.bfloat16 else torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model_qlora_base.config.use_cache = False\n",
    "    model_qlora_base.config.pad_token_id = tokenizer.pad_token_id # Set pad token ID\n",
    "\n",
    "    print(f\"Model VRAM Footprint (QLoRA Base Quantized): {model_qlora_base.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "\n",
    "    # --- Prepare for PEFT (Manual Step) ---\n",
    "    print(\"Preparing model for k-bit training and applying LoRA adapter...\")\n",
    "    model_qlora_base = prepare_model_for_kbit_training(model_qlora_base)\n",
    "    model_qlora = get_peft_model(model_qlora_base, peft_config_qlora)\n",
    "    model_qlora.print_trainable_parameters()\n",
    "\n",
    "    # --- Instantiate Standard Trainer ---\n",
    "    print(\"Instantiating Trainer...\")\n",
    "    trainer_qlora = Trainer(\n",
    "        model=model_qlora,\n",
    "        args=training_args_qlora,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        # data_collator=data_collator, # Use the standard collator\n",
    "    )\n",
    "\n",
    "    # --- Train ---\n",
    "    print(\"\\nStarting QLoRA Fine-Tuning with Trainer...\")\n",
    "    train_result_qlora = trainer_qlora.train()\n",
    "    print(\"QLoRA Fine-Tuning finished.\")\n",
    "\n",
    "    # --- Save Adapters ---\n",
    "    print(f\"Saving QLoRA adapters to {OUTPUT_DIR_FULL}...\")\n",
    "    trainer_qlora.save_model(OUTPUT_DIR_FULL) # Saves only adapters\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR_FULL)\n",
    "    print(\"Adapters and tokenizer saved.\")\n",
    "\n",
    "    # --- Plot Loss ---\n",
    "    print(trainer_qlora.state.log_history)\n",
    "    plot_loss(trainer_qlora.state.log_history, \"QLoRA (Manual Trainer) Training Loss\", f\"{OUTPUT_DIR_FULL}/qlora_manual_loss_plot\")\n",
    "\n",
    "    # --- Clean up training resources ---\n",
    "    del model_qlora_base\n",
    "    del model_qlora\n",
    "    del trainer_qlora\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Sample Generation ---\n",
    "    print(\"\\nLoading model for inference and generating sample response...\")\n",
    "    # Load base quantized model AGAIN for inference\n",
    "    inference_model_base = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16 if bnb_config.bnb_4bit_compute_dtype == torch.bfloat16 else torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    inference_model_base.config.pad_token_id = tokenizer.pad_token_id\n",
    "    qlora_model_inference = PeftModel.from_pretrained(inference_model_base, OUTPUT_DIR_FULL)\n",
    "\n",
    "    # --- Test Conversation ---\n",
    "    print(\"\\n--- Starting Test Conversation ---\")\n",
    "    current_conversation = []\n",
    "    user_prompt_1 = \"Hey, what's up? Free ho kya?\"\n",
    "    print(f\"User: {user_prompt_1}\")\n",
    "    current_conversation.append({\"role\": \"user\", \"content\": user_prompt_1})\n",
    "    response_1, current_conversation = generate_conversational_response(\n",
    "        qlora_model_inference, tokenizer, current_conversation, device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    print(f\"AI: {response_1}\")\n",
    "\n",
    "    user_prompt_2 = \"Bas class se nikla. Exam kaisa gaya tera?\"\n",
    "    print(f\"User: {user_prompt_2}\")\n",
    "    current_conversation.append({\"role\": \"user\", \"content\": user_prompt_2})\n",
    "    response_2, current_conversation = generate_conversational_response(\n",
    "        qlora_model_inference, tokenizer, current_conversation, device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    print(f\"AI: {response_2}\")\n",
    "    print(\"--- End Test Conversation ---\")\n",
    "\n",
    "    # Clean up inference model\n",
    "    del inference_model_base\n",
    "    del qlora_model_inference\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nERROR: bitsandbytes library not found. QLoRA requires it. Install: pip install bitsandbytes\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR during QLoRA Fine-Tuning: {e}\")\n",
    "    # Clean up potential partial objects\n",
    "    if 'model_qlora_base' in locals(): del model_qlora_base\n",
    "    if 'model_qlora' in locals(): del model_qlora\n",
    "    if 'trainer_qlora' in locals(): del trainer_qlora\n",
    "    if 'inference_model_base' in locals(): del inference_model_base\n",
    "    if 'qlora_model_inference' in locals(): del qlora_model_inference\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09232143",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lora_model_for_inference(base_model_id, adapter_path):\n",
    "\n",
    "    if model_name == \"Qwen-7B-LoRA\":\n",
    "        print(f\"\\nLoading {model_name} Fine-Tuned model...\")\n",
    "        # Load base model again and apply adapters\n",
    "        base_model_for_lora_inference = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            # device_map=\"auto\" # Or move manually to device\n",
    "        ).to(device)\n",
    "        base_model_for_lora_inference.config.pad_token_id = tokenizer.eos_token_id # Ensure pad token\n",
    "\n",
    "        lora_model_inference = PeftModel.from_pretrained(base_model_for_lora_inference, output_dir_lora)\n",
    "        lora_model_inference = lora_model_inference.merge_and_unload() # Merge for faster inference (optional)\n",
    "\n",
    "        return lora_model_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f3fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qlora_model_for_inference(base_model_id, adapter_path, bnb_config):\n",
    "\n",
    "    print(f\"Loading base model '{base_model_id}' with quantization...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16 if bnb_config.bnb_4bit_compute_dtype == torch.bfloat16 else torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(f\"Loading adapters from '{adapter_path}'...\")\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(\"QLoRA model loaded for inference.\")\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb90b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "output_dir_qlora = \"./Qwen2.5-7B_hinglish_finetune_with_context/Qwen2.5-7B-Instruct/qlora_finetune_epoch_5_batchsize_1\"\n",
    "try:\n",
    "    inference_model, inference_tokenizer = load_qlora_model_for_inference(MODEL_ID, output_dir_qlora, bnb_config)\n",
    "    device_inf = next(inference_model.parameters()).device # Get device from model\n",
    "\n",
    "    print(\"\\n--- Starting New Test Conversation with Loaded Model ---\")\n",
    "    chat_history = []\n",
    "    user_input = \"Kaise hai bhai?\"\n",
    "    print(f\"User: {user_input}\")\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    response, chat_history = generate_conversational_response(inference_model, inference_tokenizer, chat_history, device=device_inf)\n",
    "    print(f\"AI: {response}\")\n",
    "\n",
    "    del inference_model, inference_tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "except Exception as e:\n",
    "     print(f\"Error loading/using inference model: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
