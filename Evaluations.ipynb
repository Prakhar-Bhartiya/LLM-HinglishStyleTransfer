{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b271feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import statistics\n",
    "import re\n",
    "from collections import Counter\n",
    "from openai import OpenAI, RateLimitError, APIError\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import gc # For potential garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b80ea",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load API key securely from environment variable\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"Error: OPENAI_API_KEY environment variable not set.\")\n",
    "\n",
    "JUDGE_MODEL = \"gpt-4o\" # Or another powerful model like gpt-o1\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 5 # seconds\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a friendly college student chatbot. Respond naturally in Hinglish (mix of Romanized Hindi and English). Keep the conversation casual and engaging. Avoid being overly formal. Respond with followup questions to keep coversation engaging\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b443ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# EVALUATION PROMPTS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "JUDGE_SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert evaluator assessing chatbot responses for a Hinglish‑speaking setting.\n",
    "\n",
    "🏆 **Primary objectives (carry the most weight):**\n",
    "1. **Hindi Usage** – Response should be *Hindi‑heavy Hinglish*:  \n",
    "   • At least 40‑60 % of the tokens should be Roman‑script Hindi words / phrases.  \n",
    "   • Pure English should be used sparingly and mostly for modern slang or technical terms.  \n",
    "   • Devanagari is acceptable only for short quotes or special emphasis.  \n",
    "2. **Gender Correctness** – Any gendered address (pronouns, verb forms, honorifics) **must match the user’s stated or obvious gender**.  \n",
    "   • If gender is not explicit, default to gender‑neutral Hinglish (“yaar”, “friend”, etc.).  \n",
    "   • Mis‑gendering or inconsistent switching is a serious error.\n",
    "\n",
    "🎭 **Secondary objectives (still score, but lower weight):**\n",
    "* **Hinglish Fluency** – Natural code‑switching, idiomatic phrasing.\n",
    "* **Persona Adherence** – Friendly college student vibe: informal, relatable, campus life references.\n",
    "* **Coherence** – Logical, on‑topic, internally consistent.\n",
    "* **Engagingness** – Keeps the conversation lively / interesting.\n",
    "* **Language Constraint** – Avoids pure English or pure Hindi blocks unless context demands.\n",
    "\n",
    "Use a 1‑5 Likert scale (1 = Poor, 5 = Excellent) for each metric.  \n",
    "Provide a concise justification (1‑2 sentences) for every score.\n",
    "\n",
    "Output **only** a valid JSON object with exactly these keys:\n",
    "\n",
    "- \"hindi_usage_score\": int (1‑5)\n",
    "- \"hindi_usage_justification\": str\n",
    "- \"gender_correctness_score\": int (1‑5)\n",
    "- \"gender_correctness_justification\": str\n",
    "- \"hinglish_fluency_score\": int (1‑5)\n",
    "- \"hinglish_fluency_justification\": str\n",
    "- \"persona_adherence_score\": int (1‑5)\n",
    "- \"persona_adherence_justification\": str\n",
    "- \"coherence_score\": int (1‑5)\n",
    "- \"coherence_justification\": str\n",
    "- \"engagingness_score\": int (1‑5)\n",
    "- \"engagingness_justification\": str\n",
    "- \"language_constraint_score\": int (1‑5)\n",
    "- \"language_constraint_justification\": str\n",
    "\"\"\"\n",
    "\n",
    "JUDGE_USER_PROMPT_TEMPLATE = \"\"\"\n",
    "User Prompt:\n",
    "\"{user_prompt}\"\n",
    "\n",
    "Chatbot Response:\n",
    "\"{chatbot_response}\"\n",
    "\n",
    "Evaluate the Chatbot Response based on the criteria outlined in the system prompt.  \n",
    "Output **only** the JSON object.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74ff558",
   "metadata": {},
   "source": [
    "### Judge LLM - OpenAI 4o "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1722b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_evaluation(client: OpenAI, user_prompt: str, chatbot_response: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Gets evaluation scores from the LLM judge.\n",
    "    Returns a dictionary with scores or None if an error occurs after retries.\n",
    "    \"\"\"\n",
    "    judge_user_prompt = JUDGE_USER_PROMPT_TEMPLATE.format(\n",
    "        user_prompt=user_prompt,\n",
    "        chatbot_response=chatbot_response\n",
    "    )\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=JUDGE_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": JUDGE_SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": judge_user_prompt}\n",
    "                ],\n",
    "                temperature=0.1, # Low temperature for consistent evaluation\n",
    "                response_format={\"type\": \"json_object\"} # Request JSON output directly\n",
    "            )\n",
    "            content = response.choices[0].message.content\n",
    "            eval_data = json.loads(content)\n",
    "\n",
    "            # Basic validation of expected keys (add more checks if needed)\n",
    "            required_keys = [\n",
    "                \"hinglish_fluency_score\", \"persona_adherence_score\",\n",
    "                \"coherence_score\", \"engagingness_score\", \"language_constraint_score\"\n",
    "            ]\n",
    "            if all(key in eval_data for key in required_keys):\n",
    "                 return eval_data\n",
    "            else:\n",
    "                print(f\"Warning: LLM judge response missing required keys. Response: {content}\")\n",
    "                # Optionally treat this as an error and retry/return None\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON from LLM judge (Attempt {attempt + 1}/{MAX_RETRIES}): {e}\")\n",
    "            print(f\"LLM raw response: {content}\")\n",
    "        except (RateLimitError, APIError) as e:\n",
    "            print(f\"OpenAI API error (Attempt {attempt + 1}/{MAX_RETRIES}): {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during LLM evaluation (Attempt {attempt + 1}/{MAX_RETRIES}): {e}\")\n",
    "\n",
    "        if attempt < MAX_RETRIES - 1:\n",
    "            print(f\"Retrying in {RETRY_DELAY} seconds...\")\n",
    "            time.sleep(RETRY_DELAY)\n",
    "        else:\n",
    "            print(\"Error: Max retries reached for LLM evaluation.\")\n",
    "            return None # Failed after retries\n",
    "\n",
    "    return None # Should not be reached, but added for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e1005",
   "metadata": {},
   "source": [
    "### Quantitative Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_response_length(response: str) -> int:\n",
    "    \"\"\"Calculates the number of words in the response.\"\"\"\n",
    "    return len(response.split())\n",
    "\n",
    "def calculate_repetition_rate(response: str, n: int = 3) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the repetition rate of n-grams.\n",
    "    Args:\n",
    "        response (str): The text response.\n",
    "        n (int): The size of the n-gram (e.g., 3 for trigrams).\n",
    "    Returns:\n",
    "        float: The ratio of repeated n-grams to total n-grams. Returns 0 if not enough n-grams.\n",
    "    \"\"\"\n",
    "    words = response.lower().split()\n",
    "    if len(words) < n:\n",
    "        return 0.0\n",
    "\n",
    "    ngrams = [' '.join(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
    "    if not ngrams:\n",
    "        return 0.0\n",
    "\n",
    "    ngram_counts = Counter(ngrams)\n",
    "    repeated_ngrams = sum(1 for count in ngram_counts.values() if count > 1)\n",
    "\n",
    "    return repeated_ngrams / len(ngrams)\n",
    "\n",
    "def calculate_basic_code_switching(response: str, hindi_keywords: set, english_keywords: set) -> int:\n",
    "    \"\"\"\n",
    "    A very basic heuristic for counting potential code switches.\n",
    "    Counts transitions between likely English and likely Hindi words.\n",
    "    NOTE: This is highly approximate and prone to errors with Romanized text.\n",
    "    \"\"\"\n",
    "    words = re.findall(r'\\b\\w+\\b', response.lower())\n",
    "    if len(words) < 2:\n",
    "        return 0\n",
    "\n",
    "    switches = 0\n",
    "    current_lang = None\n",
    "\n",
    "    for word in words:\n",
    "        lang = None\n",
    "        if word in hindi_keywords:\n",
    "            lang = \"hindi\"\n",
    "        elif word in english_keywords:\n",
    "            lang = \"english\"\n",
    "        else: # (unknown word)\n",
    "            lang = None\n",
    "\n",
    "        if lang is not None:\n",
    "            if current_lang is not None and lang != current_lang:\n",
    "                switches += 1\n",
    "            current_lang = lang\n",
    "\n",
    "    return switches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a850a20b",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5eeeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    base_model_id: str,    # Base model ID\n",
    "    adapter_path: str,     # Path to the saved LoRA adapter directory\n",
    "    eval_prompts: list[str],\n",
    "    model_name: str,       # Name for this specific evaluated model/adapter\n",
    "    fine_tune_type: str              # Type : LoRA | QLoRA | Full Fine tune\n",
    "    ) -> dict:\n",
    "    \"\"\"\n",
    "    Loads a LoRA model, evaluates it using LLM judge and quantitative metrics.\n",
    "\n",
    "    Args:\n",
    "        base_model_id: Identifier for the base model on Hugging Face Hub or local path.\n",
    "        adapter_path: Path to the directory containing the trained LoRA adapter files.\n",
    "        eval_prompts: A list of user prompts for evaluation.\n",
    "        model_name: A descriptive name for the model being evaluated (e.g., \"Qwen1.5B_Hinglish_v1\").\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing aggregated evaluation results.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Evaluation for Model: {model_name} ---\")\n",
    "    print(f\"--- Base: {base_model_id}, Adapter: {adapter_path} ---\")\n",
    "\n",
    "    if not OPENAI_API_KEY:\n",
    "         print(\"Error: OpenAI API Key not configured. Cannot perform LLM evaluation.\")\n",
    "         # Optionally return partial results or raise error\n",
    "         # return {\"error\": \"OpenAI API Key not configured.\"}\n",
    "\n",
    "    # Initialize OpenAI Client (only if key is present)\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None\n",
    "\n",
    "    # Load Model and Tokenizer\n",
    "    try:\n",
    "        if fine_tune_type == \"LoRA\":\n",
    "            model, tokenizer = load_lora_model_for_inference(base_model_id, adapter_path)\n",
    "        elif fine_tune_type == \"QLoRA\":\n",
    "            print(\"Implement QLoRA\")\n",
    "            pass\n",
    "        elif fine_tune_type == \"base\":\n",
    "            model, tokenizer = load_base_model_for_inference(base_model_id)\n",
    "        else:\n",
    "            # [TODO] Full fine tune\n",
    "            print(\"Full fine tune implement\")\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model {model_name}. Aborting evaluation for this model.\")\n",
    "        return {\n",
    "            \"model_name\": model_name,\n",
    "            \"error\": f\"Model loading failed: {e}\",\n",
    "            \"per_prompt_details\": [],\n",
    "            \"aggregated_metrics\": {\"status\": \"Model Loading Failed\"}\n",
    "        }\n",
    "\n",
    "    # --- Evaluation Loop ---\n",
    "    all_results = {\n",
    "        \"model_name\": model_name,\n",
    "        \"base_model_id\": base_model_id,\n",
    "        \"adapter_path\": adapter_path,\n",
    "        \"per_prompt_details\": [],\n",
    "        \"aggregated_metrics\": {}\n",
    "    }\n",
    "\n",
    "    for i, prompt in enumerate(eval_prompts):\n",
    "        print(f\"  Processing prompt {i+1}/{len(eval_prompts)} for {model_name}...\")\n",
    "        try:\n",
    "            # 1. Prepare Chat History\n",
    "            chat_history = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "            # 2. Get Model Response\n",
    "            response_text = generate_conversational_response(model, tokenizer, chat_history)\n",
    "            if not response_text or not isinstance(response_text, str):\n",
    "                print(f\"  Warning: Invalid response received for prompt {i+1}. Using empty string.\")\n",
    "                response_text = \"\"\n",
    "\n",
    "            # 3. Calculate Quantitative Metrics\n",
    "            length = calculate_response_length(response_text)\n",
    "            repetition = calculate_repetition_rate(response_text)\n",
    "\n",
    "            # 4. Get LLM Judge Evaluation (if client is available)\n",
    "            llm_scores = None\n",
    "            if client:\n",
    "                llm_scores = get_llm_evaluation(client, prompt, response_text)\n",
    "            else:\n",
    "                llm_scores = \"Skipped (No API Key)\"\n",
    "\n",
    "\n",
    "            prompt_result = {\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": response_text,\n",
    "                \"length\": length,\n",
    "                \"repetition_rate_3gram\": repetition,\n",
    "                \"llm_evaluation\": llm_scores if llm_scores else \"Evaluation Failed or Skipped\"\n",
    "            }\n",
    "            all_results[\"per_prompt_details\"].append(prompt_result)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing prompt {i+1} for {model_name}: {e}\")\n",
    "            all_results[\"per_prompt_details\"].append({\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": f\"Error during generation or processing: {e}\",\n",
    "                \"length\": 0,\n",
    "                \"repetition_rate_3gram\": 0.0,\n",
    "                \"llm_evaluation\": \"Processing Error\"\n",
    "            })\n",
    "\n",
    "    # --- Aggregation ---\n",
    "    valid_llm_evals = [\n",
    "        res[\"llm_evaluation\"] for res in all_results[\"per_prompt_details\"]\n",
    "        if isinstance(res[\"llm_evaluation\"], dict) # Only aggregate successful LLM evals\n",
    "    ]\n",
    "    num_successful_evals = len(valid_llm_evals)\n",
    "    num_total_prompts = len(eval_prompts)\n",
    "\n",
    "    aggregated = {\n",
    "        \"status\": \"Completed\",\n",
    "        \"total_prompts\": num_total_prompts,\n",
    "        \"successful_llm_evaluations\": num_successful_evals,\n",
    "        \"failed_or_skipped_llm_evaluations\": num_total_prompts - num_successful_evals,\n",
    "    }\n",
    "\n",
    "    # Aggregate LLM Scores (if any successful evaluations)\n",
    "    if num_successful_evals > 0:\n",
    "        for key in valid_llm_evals[0].keys():\n",
    "             if key.endswith(\"_score\"):\n",
    "                metric_name = key.replace(\"_score\", \"\")\n",
    "                # Ensure score is int/float before aggregating\n",
    "                scores = [eval_data[key] for eval_data in valid_llm_evals if isinstance(eval_data.get(key), (int, float))]\n",
    "                if scores:\n",
    "                    try:\n",
    "                        aggregated[f\"avg_{metric_name}_score\"] = statistics.mean(scores)\n",
    "                        aggregated[f\"stdev_{metric_name}_score\"] = statistics.stdev(scores) if len(scores) > 1 else 0.0\n",
    "                        aggregated[f\"median_{metric_name}_score\"] = statistics.median(scores)\n",
    "                    except statistics.StatisticsError as stat_err:\n",
    "                         print(f\"Warning: Statistics error for {metric_name}: {stat_err}\")\n",
    "                         aggregated[f\"avg_{metric_name}_score\"] = None\n",
    "                else:\n",
    "                     aggregated[f\"avg_{metric_name}_score\"] = None\n",
    "\n",
    "    # Aggregate Quantitative Metrics\n",
    "    lengths = [res[\"length\"] for res in all_results[\"per_prompt_details\"] if isinstance(res.get(\"length\"), (int, float))]\n",
    "    repetitions = [res[\"repetition_rate_3gram\"] for res in all_results[\"per_prompt_details\"] if isinstance(res.get(\"repetition_rate_3gram\"), (int, float))]\n",
    "\n",
    "    aggregated[\"avg_response_length\"] = statistics.mean(lengths) if lengths else 0\n",
    "    aggregated[\"avg_repetition_rate_3gram\"] = statistics.mean(repetitions) if repetitions else 0\n",
    "\n",
    "    all_results[\"aggregated_metrics\"] = aggregated\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(f\"  Cleaning up resources for {model_name}...\")\n",
    "    del model\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"--- Evaluation Complete for Model: {model_name} ---\")\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa301a1e",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15558d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions for Model Loading & Inference ---\n",
    "\n",
    "# --- Load LoRA model ---\n",
    "def load_lora_model_for_inference(base_model_id, adapter_path):\n",
    "    \"\"\"Loads a LoRA model (base + adapters) for inference.\"\"\"\n",
    "    print(f\"Loading LoRA model: Base='{base_model_id}', Adapter='{adapter_path}'\")\n",
    "\n",
    "    if not os.path.isdir(adapter_path):\n",
    "        print(f\"Error: LoRA adapter path not found: {adapter_path}\")\n",
    "        raise FileNotFoundError(f\"LoRA adapter path not found: {adapter_path}\")\n",
    "\n",
    "    try:\n",
    "        # Load the base model\n",
    "        compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "        print(f\"Using compute dtype: {compute_dtype}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_id,\n",
    "            torch_dtype=compute_dtype,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"Base model loaded.\")\n",
    "\n",
    "        # Load the PEFT model (adapter) on top of the base model\n",
    "        model = PeftModel.from_pretrained(model, adapter_path)\n",
    "        print(\"LoRA adapter loaded.\")\n",
    "\n",
    "        # Load the tokenizer\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)\n",
    "            print(f\"Loaded tokenizer from adapter path: {adapter_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load tokenizer from adapter path '{adapter_path}'. Error: {e}. Falling back to base model '{base_model_id}'.\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "            print(f\"Loaded tokenizer from base model path: {base_model_id}\")\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            print(\"Tokenizer pad_token set to eos_token.\")\n",
    "        # Ensure model's config also knows the pad token id\n",
    "        model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        print(\"LoRA Model and Tokenizer ready.\")\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Critical Error loading model or tokenizer: {e}\")\n",
    "        raise e\n",
    "\n",
    "# --- Load base model ---\n",
    "def load_base_model_for_inference(base_model_id):\n",
    "    \"\"\"Loads only the base model and tokenizer for inference.\"\"\"\n",
    "    print(f\"Loading BASE model: '{base_model_id}'\")\n",
    "    try:\n",
    "        compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "        print(f\"Using compute dtype: {compute_dtype}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_id,\n",
    "            torch_dtype=compute_dtype,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"Base model loaded.\")\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "        print(f\"Loaded tokenizer from base model path: {base_model_id}\")\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            print(\"Tokenizer pad_token set to eos_token.\")\n",
    "        model.config.pad_token_id = tokenizer.eos_token_id # Use EOS token ID\n",
    "\n",
    "        print(\"Base Model and Tokenizer ready.\")\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Critical Error loading base model or tokenizer: {e}\")\n",
    "        raise e\n",
    "\n",
    "def generate_conversational_response(model, tokenizer, chat_history):\n",
    "    \"\"\"\n",
    "    Generates a response from the model based on the chat history.\n",
    "    Does not modify the input chat_history.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    device = model.device # PeftModel should have a device attribute after loading with device_map\n",
    "\n",
    "    # Prepare the conversation history\n",
    "    formatted_history = []\n",
    "    if chat_history and chat_history[0].get(\"role\") != \"system\":\n",
    "         # Prepend the default system prompt if not present\n",
    "         formatted_history.append({\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
    "    formatted_history.extend(chat_history)\n",
    "\n",
    "    try:\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            formatted_history,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        generation_kwargs = dict(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=400,      # Adjusted max tokens\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**generation_kwargs)\n",
    "\n",
    "        response_ids = outputs[0][input_ids.shape[-1]:]\n",
    "        response = tokenizer.decode(response_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation: {e}\")\n",
    "        response = \"Sorry yaar, kuch gadbad ho gayi generation mein. Dobara try karega?\" # Default error response\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cde749",
   "metadata": {},
   "source": [
    "### Eval LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b98c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your evaluation prompts\n",
    "evaluation_prompts = [\n",
    "    \"Hey kaise ho? College mein kya chal raha hai aajkal?\",\n",
    "    \"Yaar assignments ka bohot tension hai. Kuch tips?\",\n",
    "    \"Suggest some cool places to hangout near campus.\",\n",
    "    \"What did you think of the canteen food today?\",\n",
    "    \"Exams aa rahe hain, dar lag raha hai bhai.\",\n",
    "    \"Kal ka lecture attend kiya ya bunk maar diya?\",\n",
    "    \"Hostel ki light firse chali gayi kya?\",\n",
    "    \"Tu fest ke liye audition de raha hai kya?\",\n",
    "    \"Kya placement ka kuch update mila?\",\n",
    "    \"Online classes bore kar rahe hain, koi escape idea?\"\n",
    "]\n",
    "\n",
    "# === Define Models to Evaluate ===\n",
    "models_to_evaluate = [\n",
    "    {\n",
    "        \"model_name\": \"Qwen2.5-7B_Hinglish_base\",\n",
    "        \"base_model_id\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        \"adapter_path\": None,\n",
    "        \"fine_tune_type\" : \"base\"\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"Qwen2.5-7B_Hinglish_LoRA_LD\",\n",
    "        \"base_model_id\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        \"adapter_path\": \"./Qwen2.5-7B_hinglish_finetune_with_context/Qwen2.5-7B-Instruct/lora_finetune_7B\",\n",
    "        \"fine_tune_type\": \"LoRA\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# === Run Evaluation Loop ===\n",
    "all_evaluation_results = {}\n",
    "\n",
    "for model_info in models_to_evaluate:\n",
    "\n",
    "    results = evaluate_model(\n",
    "        base_model_id=model_info[\"base_model_id\"],\n",
    "        adapter_path=model_info[\"adapter_path\"],\n",
    "        eval_prompts=evaluation_prompts,\n",
    "        model_name=model_info[\"model_name\"],\n",
    "        fine_tune_type=model_info[\"fine_tune_type\"]\n",
    "    )\n",
    "    all_evaluation_results[model_info[\"model_name\"]] = results\n",
    "\n",
    "\n",
    "# --- Print Summary of Results ---\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"          Evaluation Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, results in all_evaluation_results.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    if \"error\" in results:\n",
    "        print(f\"  Status: Error - {results.get('status', 'Unknown Error')}\")\n",
    "        print(f\"  Details: {results['error']}\")\n",
    "    elif \"aggregated_metrics\" in results:\n",
    "        print(f\"  Status: {results['aggregated_metrics'].get('status', 'Unknown')}\")\n",
    "        print(f\"  Aggregated Metrics:\")\n",
    "        for key, value in results[\"aggregated_metrics\"].items():\n",
    "            if key != \"status\": # Don't print status twice\n",
    "                # Format floats for readability\n",
    "                if isinstance(value, float):\n",
    "                    print(f\"    {key}: {value:.3f}\")\n",
    "                else:\n",
    "                    print(f\"    {key}: {value}\")\n",
    "    else:\n",
    "            print(\"  Status: Unknown - No aggregated metrics found.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Dump full `all_evaluation_results` dictionary to a JSON file\n",
    "with open(\"evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(all_evaluation_results, f, indent=2)\n",
    "print(\"Full evaluation results saved to evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4bfb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_table(evaluation_results: Dict[str, Dict[str, Any]]) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Creates a comparison table from the evaluation results dictionary,\n",
    "    including percentage improvement over the base model.\n",
    "\n",
    "    Args:\n",
    "        evaluation_results: The dictionary containing results for multiple models.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the comparison table, ready for display\n",
    "        in Jupyter, or None if no valid results are found.\n",
    "    \"\"\"\n",
    "    table_data = []\n",
    "    # Define the primary metrics to extract initially\n",
    "    base_metric_keys = [\n",
    "        \"avg_hinglish_fluency_score\",\n",
    "        \"avg_persona_adherence_score\",\n",
    "        \"avg_coherence_score\",\n",
    "        \"avg_engagingness_score\",\n",
    "        \"avg_language_constraint_score\",\n",
    "        \"avg_gender_correctness_score\",\n",
    "        \"avg_hindi_usage_score\",\n",
    "        \"avg_response_length\",\n",
    "        \"avg_repetition_rate_3gram\",\n",
    "        \"successful_llm_evaluations\",\n",
    "        \"total_prompts\",\n",
    "    ]\n",
    "\n",
    "    # Define shorter, more readable names for table columns\n",
    "    column_names = {\n",
    "        \"model_name\": \"Model\",\n",
    "        \"eval_type\": \"Type\",\n",
    "        \"avg_hinglish_fluency_score\": \"Avg Hinglish Fluency\",\n",
    "        \"avg_persona_adherence_score\": \"Avg Persona Adherence\",\n",
    "        \"avg_gender_correctness_score\": \"Avg Gender Correctness\",\n",
    "        \"avg_hindi_usage_score\": \"Avg Hindi Usage\",\n",
    "        \"avg_coherence_score\": \"Avg Coherence\",\n",
    "        \"avg_engagingness_score\": \"Avg Engagingness\",\n",
    "        \"avg_language_constraint_score\": \"Avg Lang Constraint\",\n",
    "        \"avg_response_length\": \"Avg Length (words)\",\n",
    "        \"avg_repetition_rate_3gram\": \"Avg Repetition (3-gram)\",\n",
    "        \"successful_llm_evaluations\": \"Successful Evals\",\n",
    "        \"total_prompts\": \"Total Prompts\",\n",
    "        # Improvement columns will be added later\n",
    "    }\n",
    "\n",
    "    # --- 1. Extract Base Data and Identify Base Model ---\n",
    "    base_model_metrics = all_evaluation_results[\"Qwen2.5-3B_Hinglish_base\"]['aggregated_metrics']\n",
    "    base_model_name = \"Qwen2.5-3B_Hinglish_base\"\n",
    "    processed_results = []\n",
    "\n",
    "    for model_name, result in evaluation_results.items():\n",
    "        # Standardize result structure\n",
    "        processed_row = {\n",
    "            \"model_name\": result.get(\"model_name\", model_name),\n",
    "            \"eval_type\": result.get(\"eval_type\", \"N/A\")\n",
    "        }\n",
    "        metrics = result.get(\"aggregated_metrics\", {})\n",
    "        status = metrics.get(\"status\", \"Unknown\")\n",
    "        processed_row[\"status\"] = status\n",
    "\n",
    "        if status == \"Completed\":\n",
    "            for key in base_metric_keys:\n",
    "                processed_row[key] = metrics.get(key)\n",
    "            processed_results.append(processed_row)\n",
    "\n",
    "            if processed_row[\"eval_type\"] == \"Base Model\":\n",
    "                if base_model_metrics is not None:\n",
    "                    print(\"Warning: Multiple base models found in results. Using the first one.\")\n",
    "                else:\n",
    "                    base_model_metrics = processed_row\n",
    "                    base_model_name = processed_row[\"model_name\"]\n",
    "        else:\n",
    "            processed_row.update({key: \"Failed\" for key in base_metric_keys})\n",
    "            processed_results.append(processed_row)\n",
    "\n",
    "    if not processed_results:\n",
    "        print(\"No evaluation results found to create a table.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(processed_results)\n",
    "    df_completed = df[df['status'] == 'Completed'].copy()\n",
    "\n",
    "    # --- 2. Calculate Percentage Improvement ---\n",
    "    if base_model_metrics is None:\n",
    "        print(\"Warning: Base model results not found or failed. Cannot calculate % improvement.\")\n",
    "    else:\n",
    "        print(f\"Calculating % improvement relative to base model: '{base_model_name}'\")\n",
    "        # Metrics where higher score is better\n",
    "        metrics_higher_better = {\n",
    "            \"avg_hinglish_fluency_score\": \"Hinglish Fluency % Imp\",\n",
    "            \"avg_persona_adherence_score\": \"Persona Adherence % Imp\",\n",
    "            \"avg_gender_correctness_score\": \"Gender Correctness % Imp\",\n",
    "            \"avg_hindi_usage_score\": \"Hindi Usage % Imp\",\n",
    "            \"avg_coherence_score\": \"Coherence % Imp\",\n",
    "            \"avg_engagingness_score\": \"Engagingness % Imp\",\n",
    "            \"avg_language_constraint_score\": \"Lang Constraint % Imp\",\n",
    "        }\n",
    "        # Metrics where lower score is better\n",
    "        metrics_lower_better = {\n",
    "            \"avg_repetition_rate_3gram\": \"Repetition % Imp (Lower Better)\"\n",
    "        }\n",
    "\n",
    "        def calculate_improvement(adapter_val, base_val, lower_is_better=False):\n",
    "            adapter_val = pd.to_numeric(adapter_val, errors='coerce')\n",
    "            base_val = pd.to_numeric(base_val, errors='coerce')\n",
    "\n",
    "            if pd.isna(adapter_val) or pd.isna(base_val):\n",
    "                return np.nan\n",
    "            if base_val == 0:\n",
    "                if adapter_val == 0:\n",
    "                    return 0.0\n",
    "                return np.inf if not lower_is_better and adapter_val > 0 else (-np.inf if lower_is_better and adapter_val > 0 else np.nan)\n",
    "\n",
    "            if lower_is_better:\n",
    "                return ((base_val - adapter_val) / abs(base_val)) * 100\n",
    "            else:\n",
    "                return ((adapter_val - base_val) / abs(base_val)) * 100\n",
    "\n",
    "        for metric_key, imp_col_name in metrics_higher_better.items():\n",
    "            base_val = base_model_metrics.get(metric_key)\n",
    "            df_completed[imp_col_name] = df_completed.apply(\n",
    "                lambda row: calculate_improvement(row[metric_key], base_val, lower_is_better=False)\n",
    "                            if row[\"model_name\"] != base_model_name else np.nan,\n",
    "                axis=1\n",
    "            )\n",
    "            column_names[imp_col_name] = imp_col_name\n",
    "\n",
    "        for metric_key, imp_col_name in metrics_lower_better.items():\n",
    "            base_val = base_model_metrics.get(metric_key)\n",
    "            df_completed[imp_col_name] = df_completed.apply(\n",
    "                lambda row: calculate_improvement(row[metric_key], base_val, lower_is_better=True)\n",
    "                            if row[\"model_name\"] != base_model_name else np.nan,\n",
    "                axis=1\n",
    "            )\n",
    "            column_names[imp_col_name] = imp_col_name\n",
    "\n",
    "        improvement_cols = list(metrics_higher_better.values()) + list(metrics_lower_better.values())\n",
    "        df_improvements = df_completed[['model_name'] + improvement_cols]\n",
    "        df = pd.merge(df, df_improvements, on='model_name', how='left')\n",
    "\n",
    "    # --- 3. Final Table Formatting ---\n",
    "    df.rename(columns=column_names, inplace=True)\n",
    "\n",
    "    desired_column_order = [\n",
    "        \"Model\", \"Type\",\n",
    "        \"Avg Hinglish Fluency\", \"Hinglish Fluency % Imp\",\n",
    "        \"Avg Persona Adherence\", \"Persona Adherence % Imp\",\n",
    "        \"Avg Gender Correctness\", \"Gender Correctness % Imp\",\n",
    "        \"Avg Hindi Usage\", \"Hindi Usage % Imp\",\n",
    "        \"Avg Coherence\", \"Coherence % Imp\",\n",
    "        \"Avg Engagingness\", \"Engagingness % Imp\",\n",
    "        \"Avg Lang Constraint\", \"Lang Constraint % Imp\",\n",
    "        \"Avg Length (words)\",\n",
    "        \"Avg Repetition (3-gram)\", \"Repetition % Imp (Lower Better)\",\n",
    "        \"Successful Evals\", \"Total Prompts\",\n",
    "    ]\n",
    "    existing_desired_columns = [col for col in desired_column_order if col in df.columns]\n",
    "    df = df[existing_desired_columns]\n",
    "\n",
    "    df.set_index(\"Model\", inplace=True)\n",
    "\n",
    "    styled_df = df.style\n",
    "    float_cols = df.select_dtypes(include=['float']).columns\n",
    "    float_format_dict = {col: '{:.2f}' for col in float_cols if '% Imp' not in col}\n",
    "    percent_cols = [col for col in df.columns if '% Imp' in col]\n",
    "    percent_format_dict = {col: '{:+.1f}%' for col in percent_cols}\n",
    "    formatters = {**float_format_dict, **percent_format_dict}\n",
    "    styled_df = styled_df.format(formatters, na_rep=\"N/A\")\n",
    "\n",
    "    return styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1707971a-6ed7-47d0-a14e-c07da0d08222",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_comparison_table(all_evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b929f-0a16-4b0a-aedf-12f43c281037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "\n",
    "def transform_for_blind_test(\n",
    "    all_evaluation_results: Dict[str, Dict[str, Any]],\n",
    "    output_filename: str = \"blind_test_data.json\"\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Transforms the aggregated evaluation results into a JSON file suitable\n",
    "    for loading into the blind test application.\n",
    "\n",
    "    The output JSON will be a list of dictionaries, where each dictionary\n",
    "    represents a single valid response from a model to a specific prompt.\n",
    "    Format: [{\"prompt\": str, \"model_name\": str, \"response\": str}, ...]\n",
    "\n",
    "    Args:\n",
    "        all_evaluation_results: The dictionary containing results for multiple models,\n",
    "                                as generated by the evaluation script.\n",
    "        output_filename: The name of the JSON file to save the data to.\n",
    "\n",
    "    Returns:\n",
    "        True if the file was successfully created, False otherwise.\n",
    "    \"\"\"\n",
    "    blind_test_data = []\n",
    "    processed_models = 0\n",
    "    added_responses = 0\n",
    "\n",
    "    print(\"Starting transformation for blind test data...\")\n",
    "\n",
    "    # Iterate through each model evaluated\n",
    "    for model_key, result_data in all_evaluation_results.items():\n",
    "        model_name = result_data.get(\"model_name\", model_key) # Use specific name if available\n",
    "\n",
    "        # 1. Check if the overall model evaluation completed successfully\n",
    "        aggregated_metrics = result_data.get(\"aggregated_metrics\", {})\n",
    "        if aggregated_metrics.get(\"status\") != \"Completed\":\n",
    "            print(f\"Skipping model '{model_name}': Evaluation status was '{aggregated_metrics.get('status', 'Unknown')}'.\")\n",
    "            continue\n",
    "\n",
    "        processed_models += 1\n",
    "        print(f\"Processing model: '{model_name}'...\")\n",
    "\n",
    "        # 2. Iterate through the responses for each prompt for this model\n",
    "        per_prompt_details = result_data.get(\"per_prompt_details\", [])\n",
    "        if not per_prompt_details:\n",
    "            print(f\"  Warning: No 'per_prompt_details' found for model '{model_name}'.\")\n",
    "            continue\n",
    "\n",
    "        for detail in per_prompt_details:\n",
    "            prompt = detail.get(\"prompt\")\n",
    "            response = detail.get(\"response\")\n",
    "            llm_eval_status = detail.get(\"llm_evaluation\") # Can be dict, str, or None\n",
    "\n",
    "            # 3. Validate the response and its evaluation status\n",
    "            is_valid_response = isinstance(response, str) and response.strip() and \\\n",
    "                                \"Error during generation\" not in response and \\\n",
    "                                \"Processing Error\" not in response\n",
    "\n",
    "            # Consider evaluation valid if it's a dict (success) or skipped due to no API key\n",
    "            is_valid_evaluation = isinstance(llm_eval_status, dict) or \\\n",
    "                                  llm_eval_status == \"Skipped (No API Key)\" or \\\n",
    "                                  llm_eval_status == \"Evaluation Failed or Skipped\" # Allow this status from previous code\n",
    "\n",
    "\n",
    "            if prompt and is_valid_response and is_valid_evaluation:\n",
    "                # 4. Add valid data point to the list\n",
    "                blind_test_data.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"model_name\": model_name,\n",
    "                    \"response\": response\n",
    "                })\n",
    "                added_responses += 1\n",
    "            # Optionally log skipped responses\n",
    "            # else:\n",
    "            #     print(f\"  Skipping response for prompt '{prompt[:30]}...' - Invalid response or LLM eval status.\")\n",
    "\n",
    "\n",
    "    print(f\"\\nProcessed {processed_models} completed model evaluations.\")\n",
    "    print(f\"Extracted {added_responses} valid prompt-response pairs for blind testing.\")\n",
    "\n",
    "    if not blind_test_data:\n",
    "        print(\"Warning: No valid data found to write to the blind test file.\")\n",
    "        return False\n",
    "\n",
    "    # 5. Write the collected data to the JSON file\n",
    "    try:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(blind_test_data, f, indent=2, ensure_ascii=False) # Use indent for readability\n",
    "        print(f\"Successfully saved blind test data to '{output_filename}'\")\n",
    "        return True\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing blind test data to file '{output_filename}': {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during file writing: {e}\")\n",
    "        return False\n",
    "\n",
    "# Call the function with your results dictionary\n",
    "output_file = \"blind_test_data_output.json\"\n",
    "success = transform_for_blind_test(all_evaluation_results, output_file)\n",
    "\n",
    "if success:\n",
    "    # You can optionally print the first few records to verify\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            print(\"\\n--- First few records from generated JSON ---\")\n",
    "            for i, record in enumerate(data[:3]): # Print first 3 records\n",
    "                print(f\"Record {i+1}:\")\n",
    "                print(f\"  Prompt: {record['prompt'][:60]}...\") # Truncate long prompts\n",
    "                print(f\"  Model: {record['model_name']}\")\n",
    "                print(f\"  Response: {record['response'][:80]}...\") # Truncate long responses\n",
    "            if len(data) > 3:\n",
    "                print(\"...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading back generated file for verification: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
